{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Solution_Backend_Summit_Workshop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWad20A_F8Eo",
        "colab_type": "text"
      },
      "source": [
        "# Back-end summit Machine Learning workshop 2019\n",
        "\n",
        "The goal of this workshop is to learn about Machine Learning by going through the model building process step by step. The goal is **_not_** to build the most accurate model, but rather understand what steps are involved to get to your first minimal viable model. Building a good machine learning model is an iterative process, this workshop is a good starting point for engineers who want to get a first try at practical machine learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp3pfMtDF8Ep",
        "colab_type": "text"
      },
      "source": [
        "### Business case\n",
        "\n",
        "Every machine learning project starts with a business problem. FC ING is bankcrupt and therefore the board has decided to sell all their players. Certainly they want to get the best deal, but they have not been up to date with the recent developments in the soccer industry. Thats why they need your help! \n",
        "Develop a Machine Learning model that:\n",
        "\n",
        "- ### Predicts the value of a football player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G18z-Repii_0",
        "colab_type": "text"
      },
      "source": [
        "In order to successfully build your first machine learning model, you just have to complete this Jupyter Notebook by following the instructions in the code cells. Please write your code in Python and use the pandas, numpy or sklearn library for help. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdMSzAZrF8Eq",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://ichef.bbci.co.uk/news/660/cpsprodpb/CF91/production/_103573135_neymareasports.jpg\" alt=\"Drawing\" style=\"width: 100px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXiMFL1gF8Er",
        "colab_type": "text"
      },
      "source": [
        "### The dataset\n",
        "\n",
        "The FIFA 2019 data set is an open source data set that contains detailed attributes of well-known soccer players (https://www.kaggle.com/karangadiya/fifa19).\n",
        "The target variable that we want to predict is the \"value\" column. Everything else you can use as input/features for your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73ELGwXmF8Er",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell before you do anything else\n",
        "\n",
        "# Pandas is a very handy Python library that helps you to load, clean, analyse and preprocess your data before you build a model.\n",
        "import pandas as pd \n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "\n",
        "# These are all the imports that you could require one way or another to complete the notebook\n",
        "# Of course, there might be libraries not imported and you would like to use, please add them here\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "\n",
        "# Model imports\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import SGDRegressor, Lasso, Ridge, LinearRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYKyWsWSF8Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the dataset into a pandas Dataframe and print the first 5 rows to get a first glance at the data.\n",
        "raw_data = pd.read_csv(\"https://drive.google.com/uc?authuser=0&id=1XomAUds7vJ2aA2Rde0LAaKto3MldAeTH&export=download\", index_col=0)\n",
        "raw_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz66NGYJF8Ew",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing the data\n",
        "\n",
        "(Big) data often comes in messy and unstructured format. For this workshop we choose a rather clean dataset, but regardless you have to do some pre-processing to get the features ready for modeling.\n",
        "Here we will do 3 types of pre-processing:\n",
        "- Dropping of redundant columns\n",
        "- Transforming non-numeric to numeric values\n",
        "- Deal with NaN values (missing values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb_nPnssx6OK",
        "colab_type": "text"
      },
      "source": [
        "### Dropping of redundant columns\n",
        "\n",
        " Drop the \"Nationality\" column, we don't need this data.  \n",
        "Hint: use the the drop method of the pandas Dataframe , https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmjW9BBcF8Ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_nat_data = raw_data.drop(columns=[\"Nationality\"])\n",
        "no_nat_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9ShU-sTyWlQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Transforming non-numeric to numeric values\n",
        "Transform the \"Weight\" column from lbs string to a kg float.\n",
        "This method takes as input a string that looks like: 123lbs\n",
        "and should output a float like: 55.79"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjstK5PIgOto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_weight(weight:str) -> np.float64:\n",
        "  weight_trans = weight.replace(\"lbs\", \"\")\n",
        "  return float(weight_trans) * 0.45\n",
        "\n",
        "weight = \"123lbs\"\n",
        "transform_weight(weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSaicDghypLW",
        "colab_type": "text"
      },
      "source": [
        "Run this cell after completing the transform_weight method in the previous cell. The next cell will apply your implemented method to the data in the correct way and will print some validation information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-M7DFpKF3ur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a deep copy of the data\n",
        "weight_trans_data = no_nat_data.copy()\n",
        "\n",
        "print(\"Data type of weight column before transformation is: {}\".format(weight_trans_data[\"Weight\"].dtype))\n",
        "print()\n",
        "\n",
        "# Apply method to a Series(indexed list) of data\n",
        "weight_trans_data[\"Weight\"] = no_nat_data[\"Weight\"].apply(transform_weight)\n",
        "print(weight_trans_data[\"Weight\"].head())\n",
        "print(\"\\nData type of weight column after transformation should be float64 and is: {}\".format(weight_trans_data[\"Weight\"].dtype))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWbMtmB0F8E5",
        "colab_type": "text"
      },
      "source": [
        "### Deal with missing values (NaN)\n",
        "\n",
        "Often you have to deal with missing values in your dataset. Among many different methods you can replace missing values with the mean of the feauture column. \n",
        "\n",
        "For example, each player that has height missing, we assign the average height of the rest of the players.\n",
        "\n",
        "The following cell will explore which columns have missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo73Zn9UKBi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Counting all non null values in different columns: \")\n",
        "# Sums up the count of null values in each column in the DataFrame\n",
        "nulls = weight_trans_data.isna().sum()\n",
        "print(nulls)\n",
        "\n",
        "# Shows all columns that have at least 1 nan value\n",
        "print(\"\\nShowing columns with nan values: \")\n",
        "nulls.iloc[nulls.to_numpy().nonzero()[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dht0d5VLzvQz",
        "colab_type": "text"
      },
      "source": [
        "Fill the found NaN values with the average weight of the players.  \n",
        "\n",
        "Use the Dataframe fill_na method https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.fillna.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BydyoFc9R8cV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "height_filled_data = weight_trans_data.copy()\n",
        "# Inspect data before transformation\n",
        "print(\"Before transformation: \\n\")\n",
        "print(height_filled_data[\"Height\"].head())\n",
        "\n",
        "# Apply transformation\n",
        "mean = height_filled_data[\"Height\"].mean()\n",
        "height_filled_data[\"Height\"] = height_filled_data[\"Height\"].fillna(value=mean)\n",
        "\n",
        "print(\"\\nAfter transformation: \\n\")\n",
        "print(height_filled_data[\"Height\"].head())\n",
        "\n",
        "print(\"\\nColumns with nan values after imputing: \")\n",
        "new_nulls = height_filled_data.isna().sum()\n",
        "new_nulls.iloc[new_nulls.to_numpy().nonzero()[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIZ_NTSMF8E1",
        "colab_type": "text"
      },
      "source": [
        "### Exploratory Data Analysis\n",
        "\n",
        "In this step you want to explore your data, ask questions, find patterns and maybe create visualisation. Usually this is an iterative process which you come back to throughout a Machine Learning project. It is a really valuable step to get to know your dataset. For example:\n",
        "\n",
        "1. How is \"Age\" distributed among the players?\n",
        "2. Do players with a high \"Special\" number have a greater \"Value\"?\n",
        "3. Is the \"Overall\" score of a player related to his \"Preferred Foot\"?\n",
        "\n",
        "Tip:\n",
        "1. For a distribution plot have a look at https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n",
        "2. To see how two parameters relate to each other use: https://seaborn.pydata.org/generated/seaborn.scatterplot.html\n",
        "3. See 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "jF09S98kF8E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 \n",
        "sns.kdeplot(raw_data[\"Age\"], shade=True)\n",
        "plt.figure()\n",
        "#2\n",
        "sns.scatterplot(x=\"Special\", y=\"Value\", data=raw_data)\n",
        "plt.figure()\n",
        "#3a\n",
        "sns.scatterplot(x=\"Overall\", y=\"Value\", hue=\"Preferred Foot\", data=raw_data)\n",
        "plt.figure()\n",
        "#3b\n",
        "d = raw_data.copy()\n",
        "d[\"Value\"] = d[\"Value\"].apply(np.log)\n",
        "sns.scatterplot(x=\"Overall\", y=\"Value\", hue=\"Preferred Foot\", data=d)\n",
        "plt.figure()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg5MmG6TF8E6",
        "colab_type": "text"
      },
      "source": [
        "### Feature selection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPfc6ZvkH1gF",
        "colab_type": "text"
      },
      "source": [
        "Correlation and regression analysis are related in the sense that both deal with relationships among variables. The correlation coefficient is a measure of linear association between two variables. Values of the correlation coefficient are always between -1 and +1. A correlation coefficient of +1 indicates that two variables are perfectly related in a positive linear sense, a correlation coefficient of -1 indicates that two variables are perfectly related in a negative linear sense, and a correlation coefficient of 0 indicates that there is no linear relationship between the two variables. \n",
        "\n",
        "We use this concept to get an idea of what features to select for our model.\n",
        "\n",
        "\n",
        "**Example:** \n",
        "If the age of a player is negatively correlated to the value of a player, it might be a good predictor for our model because when age increases/decreases the value of a player decreases/increases, respectively.\n",
        "\n",
        "## Correlation analysis\n",
        "\n",
        "Print a sorted list with the greatest absolute correlation values of all features compared to the \"Value\" to get an indication of what\n",
        "are the best features for predicting a player's value.\n",
        "\n",
        "Tip:\n",
        "- DataFrame.corr https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "- Select K best features: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clyvq1IuH11Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set number of features to show\n",
        "n = 40\n",
        "\n",
        "non_value_columns = height_filled_data.columns.drop(\"Value\")\n",
        "correlations = height_filled_data.corr()['Value'][non_value_columns]\n",
        "abs_sorted_correlations = correlations.apply(abs).sort_values(ascending = False)[:n]\n",
        "print(abs_sorted_correlations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDMUns5jF8E6",
        "colab_type": "text"
      },
      "source": [
        "### Build your first model\n",
        "\n",
        "Now we are ready to build our first model! You never train the model on all of your data. You want to partition your data in 80% training data and 20% test data.  \n",
        "The training data is used to train the model. Once you trained a model you should evaluate how good it is using the test set. You evaluate a model by exposing it to unseen data. This is why you always set aside a test set. \n",
        "\n",
        "**Tip**: \n",
        "1. Select numeric data: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html\n",
        "2. Split data: [sklearn.model_selection.train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv6cwZafyebM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The model target is the Value of our players we are trying to estimate\n",
        "target = height_filled_data[\"Value\"]\n",
        "# The rest of the columns are your features, but make sure we only use numeric features, as we can only use numeric values in our models.\n",
        "\n",
        "# 1 \n",
        "num_features = height_filled_data.select_dtypes(exclude='object')\n",
        "\n",
        "# Also, make sure to drop Value as a feature, as this would obviously not be available in our \"real world\" data\n",
        "features = num_features.drop(columns=[\"Value\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQb6E5ZXwv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2\n",
        "# Finally split the data into training and testing set\n",
        "train_X, test_X, train_y, test_y = train_test_split(features, target, test_size = 0.20, random_state =1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo1fhgKOvRV3",
        "colab_type": "text"
      },
      "source": [
        "# Model training\n",
        "Train your model (on the training data) with a regression algorithm of your choice. \n",
        "\n",
        "Choosing what model to use for modeling the problem at hand can be challenging. In order to guide you,\n",
        "please use the following guide to help you get started: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html.  \n",
        "After you picked the correct regression model, please instantiate it, and assign it to the \"clf\" variable.  \n",
        "\n",
        "**Example**:   \n",
        "clf = LinearRegression()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmIhuCmFvnqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = RandomForestRegressor()\n",
        "\n",
        "# Train the model on training data\n",
        "clf.fit(train_X, train_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdegw6xJF8E7",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation of the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CXt_PpMwFVT",
        "colab_type": "text"
      },
      "source": [
        "Now that you trained a model, you want to investigate how well it will do on new unseen data. That is why we always hold out a small partion of the data, called the test set.\n",
        "There are many different evaluation metrics in machine learning. Depending on the algorithm you use you choose this metric.\n",
        "For simplicity, we will use the Root Mean Square Error (RMSE). When we talk about model we actually mean fitting a line/function to our training data by minimizing the error distance of that line and the datapoints. The RMSE is the distance, on average, of a data point from the fitted line, measured along a vertical line. That is probably the most easily interpreted statistic, since it has the same units as the quantity plotted on the vertical axis. In our case that is the value in â‚¬. Thus, the smaller the RMSE, the better your model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGXecAJt3Bn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper method to pretty print results about your model\n",
        "def print_predictions(pred, real) -> None:\n",
        "  # Prepare for concatenation\n",
        "  pred_array = pred.reshape(-1,1)\n",
        "  real_array = real.to_numpy().reshape(-1,1)\n",
        "  \n",
        "  # Concatenate real value, predictions and difference as a pandas DataFrame\n",
        "  diff = abs(pred_array - real_array)\n",
        "  results = np.concatenate((real_array, pred_array, diff), axis=1)\n",
        "  df_results = pd.DataFrame(results, columns=[\"Real value\", \"Predicted value\", \"Difference\"])\n",
        "\n",
        "  print(\"Showing difference in predicted and real value for first 5 players: \\n\")\n",
        "  print(df_results.head())\n",
        "\n",
        "  print(\"\\nListing difference in predicted and real value sorting by greatest difference: \\n\")\n",
        "  print(df_results.sort_values(by=\"Difference\", ascending=False).head())\n",
        "\n",
        "  # Calculate the RMSE\n",
        "  rmse = sqrt(mean_squared_error(real_array, pred_array))\n",
        "  print(\"\\n Model score: \\n\")\n",
        "  print(rmse)\n",
        "\n",
        "# Use the model's predict method on the test data\n",
        "predictions = clf.predict(test_X)\n",
        "\n",
        "print_predictions(predictions, test_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzGT2jSGxdoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}